{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ssk5BSNbelCF"
   },
   "source": [
    "# Word2Vec實作\n",
    "- 字詞所代表的意義非常多元，在不同狀況下，會代表不同意思。要把多元意思用單一向量表示，則必須要進行word embedding的動作，也就是把高維向量降為低維向量的過程\n",
    "- 之前介紹過，利用分散式表示法來表達字詞向量，例如PMI、SVD..統計法..等\n",
    "- 2013年神經網路盛行後，Tomas Mikolov利用神經網路訓練方式，來獲得字詞的表達向量，獲得很棒的成果。一般認為是利用神經網路模擬人類的理解能力，獲得不錯的分布空間所得到的成果。\n",
    "- 本範例以維基百科wiki部分資料作範例\n",
    "- 資料來源：https://dumps.wikimedia.org/zhwiki/20231201/zhwiki-20231201-pages-articles-multistream1.xml-p1p187712.bz2\n",
    "- 利用結巴分詞(jieba)進行斷詞，gensim套件進行word2vec計算\n",
    "- 本範例約需1小時長時間執行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ylVzFulsmgzB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-04 20:46:12--  https://dumps.wikimedia.org/zhwiki/20231201/zhwiki-20231201-pages-articles-multistream1.xml-p1p187712.bz2\n",
      "Resolving proxy.server (proxy.server)... 10.0.0.203\n",
      "Connecting to proxy.server (proxy.server)|10.0.0.203|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 230609687 (220M) [application/octet-stream]\n",
      "Saving to: ‘zhwiki-20231201-pages-articles-multistream1.xml-p1p187712.bz2’\n",
      "\n",
      "zhwiki-20231201-pag 100%[===================>] 219.93M  3.98MB/s    in 56s     \n",
      "\n",
      "2024-01-04 20:47:08 (3.91 MB/s) - ‘zhwiki-20231201-pages-articles-multistream1.xml-p1p187712.bz2’ saved [230609687/230609687]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dumps.wikimedia.org/zhwiki/20231201/zhwiki-20231201-pages-articles-multistream1.xml-p1p187712.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMIti7ru3701"
   },
   "source": [
    "### opencc是繁簡轉換工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0Ta_qze2iHJi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: opencc-python-reimplemented in ./.local/lib/python3.11/site-packages (0.1.7)\n"
     ]
    }
   ],
   "source": [
    "# 使用 pip 安裝 opencc-python-reimplemented 套件\n",
    "!pip install opencc-python-reimplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQCWrmFJ4Mps"
   },
   "source": [
    "### gensim是訓練word2vec的函式庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1BIoy7K5eqIS"
   },
   "outputs": [],
   "source": [
    "# 建立 WikiCorpus 物件，讀取中文維基百科檔案（請確保已經下載並解壓縮）\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "# 建立 WikiCorpus 物件，讀取中文維基百科檔案（請確保已經下載並解壓縮）\n",
    "wiki_corpus = WikiCorpus('zhwiki-20231201-pages-articles-multistream1.xml-p1p187712.bz2', dictionary={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0T7ZZpDr3Sg8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.wikicorpus.WikiCorpus at 0x7fb51ced9410>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KqX5g3Idsp-V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['歐幾里得',\n",
       " '西元前三世紀的古希臘數學家',\n",
       " '現在被認為是幾何之父',\n",
       " '此畫為拉斐爾的作品',\n",
       " '雅典學院',\n",
       " '数学',\n",
       " '是研究數量',\n",
       " '屬於形式科學的一種',\n",
       " '數學利用抽象化和邏輯推理',\n",
       " '從計數']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取得 WikiCorpus 的文字內容，使用 iter 取得迭代器，再用 next 取得下一個元素\n",
    "# 並使用切片 [:10] 取得該元素的前十個元素\n",
    "next(iter(wiki_corpus.get_texts()))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjOTHZGR5nCE"
   },
   "source": [
    "## 把wiki的資料檔案，轉換成連續文字的txt檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AL99YGiSfhGu"
   },
   "outputs": [],
   "source": [
    "# 初始化計數器 text_num，用來計算處理的文章數量\n",
    "text_num = 0\n",
    "\n",
    "# 開啟檔案 'wiki_text.txt' 以進行寫入，使用 utf-8 編碼\n",
    "with open('wiki_text.txt', 'w', encoding='utf-8') as f:\n",
    "    # 迭代 WikiCorpus 中的每一篇文章\n",
    "    for text in wiki_corpus.get_texts():\n",
    "        # 將文章中的字詞用空格連接，寫入文件，並換行\n",
    "        f.write(' '.join(text)+'\\n')\n",
    "        # 計數\n",
    "        text_num += 1\n",
    "        # 每處理 10000 筆資料，顯示進度訊息\n",
    "        if text_num % 10000 == 0:\n",
    "            print('{} articles processed.'.format(text_num))\n",
    "\n",
    "    print('{} articles processed.'.format(text_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQ01OEi6szXr"
   },
   "outputs": [],
   "source": [
    "# 引入 jieba 和 opencc 套件\n",
    "!pip install jieba\n",
    "import jieba\n",
    "from opencc import OpenCC\n",
    "\n",
    "\n",
    "# 初始化 OpenCC，將簡體中文轉換為繁體中文\n",
    "cc = OpenCC('s2t')\n",
    "# 讀取 'wiki_text.txt' 文本檔案的內容\n",
    "train_data = open('wiki_text.txt', 'r', encoding='utf-8').read()\n",
    "# 使用 OpenCC 將簡體中文轉換為繁體中文\n",
    "train_data = cc.convert(train_data)\n",
    "# 使用 jieba 套件進行中文分詞\n",
    "train_data = jieba.lcut(train_data)\n",
    "# 去除空白字詞\n",
    "train_data = [word for word in train_data if word != '']\n",
    "# 將分詞後的字詞用空格連接成字串\n",
    "train_data = ' '.join(train_data)\n",
    "# 將處理後的結果寫入 'seg.txt' 文本檔案\n",
    "open('seg.txt', 'w', encoding='utf-8').write(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXxRBbays3k0"
   },
   "outputs": [],
   "source": [
    "# 引入 gensim 中的 word2vec\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "# 設定 Word2Vec 的相關參數\n",
    "seed = 666\n",
    "sg = 0\n",
    "window_size = 10\n",
    "#vector_size = 100\n",
    "min_count = 1\n",
    "workers = 8\n",
    "#epochs = 5\n",
    "batch_words = 10000\n",
    "\n",
    "# 讀取斷詞後的資料\n",
    "train_data = word2vec.LineSentence('seg.txt')\n",
    "# 使用 Word2Vec 訓練模型\n",
    "model = word2vec.Word2Vec(\n",
    "    train_data,\n",
    "    min_count=min_count,\n",
    "    #size=vector_size,\n",
    "    workers=workers,\n",
    "    #iter=epochs,\n",
    "    window=window_size,\n",
    "    sg=sg,\n",
    "    seed=seed,\n",
    "    batch_words=batch_words\n",
    ")\n",
    "\n",
    "# 儲存訓練好的 Word2Vec 模型\n",
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PovTacQs-R_"
   },
   "outputs": [],
   "source": [
    "# 引入 gensim 中的 word2vec\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 指定詞彙\n",
    "string = '電腦'\n",
    "# 載入訓練好的 Word2Vec 模型\n",
    "model = word2vec.Word2Vec.load('word2vec.model')\n",
    "# 列印指定詞彙\n",
    "print(string)\n",
    "\n",
    "# 顯示與指定詞彙最相似的詞彙及其相似度\n",
    "for item in model.wv.most_similar(string):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E235am9EVIuG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
